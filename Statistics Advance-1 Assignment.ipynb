{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe15cb4b-c0fa-4888-ba49-0472c8b0ab1f",
   "metadata": {},
   "source": [
    "# Statistics Advance-1 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c52ff-ac91-4284-b44e-6d99b2c31ea9",
   "metadata": {},
   "source": [
    "## 1. Explain the properties of the F-distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f4d2e-3064-42c0-9c27-5c77120c0a26",
   "metadata": {},
   "source": [
    "properties of the F-distribution:\n",
    "Asymmetrical: The F-distribution is skewed to the right and is not symmetrical.\n",
    "Defined by two parameters: The F-distribution is defined by the degrees of freedom of the numerator (\\(m\\)) and the denominator (\\(n\\)).\n",
    "Positive values: The F-distribution can only have positive values.\n",
    "Approximates the normal distribution: As the degrees of freedom for the numerator and denominator increase, the F-distribution approximates the normal distribution.\n",
    "Uses: The F-distribution is used to compare two variances and is associated with the analysis of variance (ANOVA) and the F-test.\n",
    "Bounded on the left: The distribution is bounded on the left by zero and has no upper limit.\n",
    "Probability density function (PDF): The PDF of the F-distribution is more complex than that of other distributions, such as the normal or t-distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7d0d2-a70a-4b24-9d70-1ee7e908b509",
   "metadata": {},
   "source": [
    "# 2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa8a41-b178-4d21-9ee8-f4ae8f8bb3b2",
   "metadata": {},
   "source": [
    "The F-distribution is used in statistical tests that involve comparing variances, group means, or model fits. Its appropriateness stems from its definition as a ratio of two scaled variances, which makes it ideal for analyzing variability. Here are the main types of statistical tests that use the F-distribution and why it is suitable for each:\n",
    "\n",
    "1. Analysis of Variance (ANOVA)\n",
    "Purpose: To test whether there are significant differences between the means of three or more groups.\n",
    "Why the F-distribution is appropriate:\n",
    "ANOVA compares the variance between groups (due to differences in group means) to the variance within groups (due to random variation).\n",
    "The F-statistic is calculated as the ratio of these two variances, and under the null hypothesis (no group differences), it follows an F-distribution.\n",
    "\n",
    "2. Regression Analysis\n",
    "Purpose: To test the significance of regression models and predictors.\n",
    "Why the F-distribution is appropriate:\n",
    "The F-test compares the variance explained by the model (regression sum of squares) to the unexplained variance (residual sum of squares).\n",
    "It assesses whether the overall model provides a significantly better fit to the data than a model with no predictors.\n",
    "\n",
    "3. Test for Equality of Variances (Leveneâ€™s or Bartlettâ€™s Test)\n",
    "Purpose: To determine if two or more populations have equal variances.\n",
    "Why the F-distribution is appropriate:\n",
    "The test statistic involves comparing the sample variances, which is naturally expressed as a ratio.\n",
    "The ratio of two variances follows the F-distribution under the null hypothesis of equal variances.\n",
    "\n",
    "4. Comparison of Nested Models\n",
    "Purpose: To determine whether adding complexity to a statistical model significantly improves its performance.\n",
    "Why the F-distribution is appropriate:\n",
    "The F-test compares the reduction in error achieved by the more complex model relative to the increase in parameters.\n",
    "This ratio is modeled as an F-distribution under the null hypothesis.\n",
    "\n",
    "5. Multivariate Analysis of Variance (MANOVA)\n",
    "Purpose: To test differences in multiple dependent variables across groups.\n",
    "Why the F-distribution is appropriate:\n",
    "MANOVA extends ANOVA by comparing variance-covariance matrices and uses an F-statistic to assess differences.\n",
    "\n",
    "6. Two-Factor ANOVA (Without Replication)\n",
    "Purpose: To test the effects of two independent variables (factors) on a dependent variable.\n",
    "\n",
    "Why the F-distribution is appropriate:\n",
    "Variance is partitioned into components attributable to each factor and their interaction, and F-tests are used to evaluate their significance.\n",
    "Why the F-Distribution Works for These Tests\n",
    "Derived from Variances: Since the F-distribution is the ratio of two variances, it is naturally suited for tests comparing variability.\n",
    "Skewness Matches Nature of Variance Ratios: Variance ratios are non-negative and often skewed, aligning with the properties of the F-distribution.\n",
    "Null Hypothesis Behavior: Under the null hypothesis (e.g., no group differences or equal variances), the test statistic follows an F-distribution, allowing for accurate probability calculations.\n",
    "By using the F-distribution, these tests ensure valid inference about relationships between variances, group differences, and model fits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a2bd8-a37e-4e22-bba0-d74a4467fa32",
   "metadata": {},
   "source": [
    "# 3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665adc7-3fbd-428f-a232-c8d6a9a3745b",
   "metadata": {},
   "source": [
    "The assumptions are as follows:\n",
    "\n",
    "1. Independent Samples\n",
    "The two samples must be independent, meaning the data in one sample should not influence or be related to the data in the other sample.\n",
    "\n",
    "2. Normally Distributed Populations\n",
    "The populations from which the samples are drawn should follow a normal distribution.\n",
    "The F-test is highly sensitive to departures from normality, especially when sample sizes are small. If normality cannot be ensured, alternative tests like Leveneâ€™s test or the Brown-Forsythe test may be more appropriate.\n",
    "\n",
    "3. Random Sampling\n",
    "The samples should be randomly drawn from the populations to avoid bias.\n",
    "\n",
    "4. Ratio of Variances is Positively Distributed\n",
    "Since the F-statistic involves a ratio of variances, the values must be non-negative, aligning with the properties of variances.\n",
    "\n",
    "5. Homogeneity of Variances Within Each Population\n",
    "While the F-test specifically tests for equality of variances, it assumes that any subgrouping within each population does not itself have varying variances.\n",
    "\n",
    "6. Non-Degenerate Data\n",
    "The variances of the two samples must be finite and not zero. Zero variance would make the ratio undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a157212-79ee-427f-80a7-077a146aacc7",
   "metadata": {},
   "source": [
    "# 4. What is the purpose of ANOVA, and how does it differ from a t-test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943f8f8-5fc1-4d40-9d65-7ef225d3961d",
   "metadata": {},
   "source": [
    "The purpose of ANOVA (Analysis of Variance) is to test whether there are statistically significant differences between the means of three or more groups. It evaluates how much of the total variability in the data is due to differences between group means versus random variation within the groups.\n",
    "\n",
    "Purpose of ANOVA:\n",
    "1.Testing Group Differences:\n",
    "ANOVA determines if at least one group mean is significantly different from the others.\n",
    "\n",
    "2. Partitioning Variance:\n",
    "It separates the variability in the data into:\n",
    "Between-group variance: Variability due to differences between group means.\n",
    "Within-group variance: Variability due to differences within the groups (random noise or error).\n",
    "\n",
    "3.Hypothesis Testing:\n",
    "Null Hypothesis (ð»0): All group means are equal (Î¼1 = Î¼2 = Î¼3 =â€¦).\n",
    "Alternative Hypothesis (ð»ð‘Ž): At least one group mean is different.\n",
    "\n",
    "how does it differ from a t-test:\n",
    "1. Number of Groups:\n",
    "A t-test compares the means of two groups.\n",
    "ANOVA compares the means of three or more groups.\n",
    "\n",
    "2. Purpose:\n",
    "A t-test checks if there is a significant difference between two means.\n",
    "ANOVA checks if at least one group mean is different among multiple groups.\n",
    "\n",
    "3. Test Statistic:\n",
    "The t-test uses the t-statistic.\n",
    "ANOVA uses the F-statistic, which compares variances between and within groups.\n",
    "\n",
    "4. Error Control:\n",
    "Multiple t-tests increase the risk of Type I error.\n",
    "ANOVA controls the overall Type I error rate when comparing multiple groups.\n",
    "\n",
    "5. Post-hoc Analysis:\n",
    "A t-test does not require post-hoc tests.\n",
    "ANOVA requires post-hoc tests (e.g., Tukeyâ€™s) to determine which groups differ if the result is significant.\n",
    "\n",
    "6. Flexibility:\n",
    "A t-test is limited to simple comparisons between two groups.\n",
    "ANOVA is suitable for analyzing multiple groups or complex designs like factorial or repeated measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a190e0c-3477-48a4-8b91-4e0abcb43c70",
   "metadata": {},
   "source": [
    "# 5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bb140-aaeb-49a0-8f41-bd921f168ff4",
   "metadata": {},
   "source": [
    "1. Control of Type I Error\n",
    "Why:\n",
    "Conducting multiple t-tests increases the risk of a Type I error (false positive). For example, if you compare 3 groups with t-tests, you perform 3 comparisons, each with its own 5% significance level, compounding the error rate.\n",
    "How ANOVA helps:\n",
    "One-way ANOVA performs a single test to determine if there are any group differences, keeping the overall Type I error rate at the desired significance level (e.g., 5%).\n",
    "\n",
    "2. Efficiency\n",
    "Why:\n",
    "Comparing multiple groups using individual t-tests is inefficient, requiring a separate test for each pair of groups.\n",
    "How ANOVA helps:\n",
    "One-way ANOVA evaluates all group means simultaneously in a single test, saving time and computational effort.\n",
    "\n",
    "3. Comprehensive Results\n",
    "Why:\n",
    "Multiple t-tests do not provide an overarching view of the data and only evaluate pairwise differences.\n",
    "How ANOVA helps:\n",
    "One-way ANOVA identifies whether any significant differences exist among the groups without needing multiple comparisons.\n",
    "\n",
    "4. Proper Statistical Framework\n",
    "Why:\n",
    "T-tests are not designed for multi-group comparisons and may violate statistical assumptions when used for this purpose.\n",
    "How ANOVA helps:\n",
    "One-way ANOVA is specifically designed to compare multiple groups while partitioning variance into between-group and within-group components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c485d-5f2c-4cdd-acd5-fc5969c254ef",
   "metadata": {},
   "source": [
    "# 6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a2b4e-90c6-4127-8ea7-95cb1cf60864",
   "metadata": {},
   "source": [
    "In ANOVA, the total variance in a dataset is partitioned into two components:\n",
    "\n",
    "Between-Group Variance: \n",
    "This measures the variability between the means of different groups. It represents the differences in the average values of the dependent variable across the different groups.\n",
    "Within-Group Variance: \n",
    "This measures the variability within each group. It represents the differences among individuals within the same group.\n",
    "How does this partitioning contribute to the calculation of the F-statistic?\n",
    "\n",
    "The F-statistic is a ratio of two variances:\n",
    "F-statistic = Between-Group Variance / Within-Group Variance\n",
    "\n",
    "Numerator (Between-Group Variance): \n",
    "If the means of the groups are significantly different, the between-group variance will be large. This indicates that the independent variable has a significant effect on the dependent variable.\n",
    "Denominator (Within-Group Variance): \n",
    "This represents the inherent variability within each group, which is assumed to be random and unrelated to the independent variable.\n",
    "\n",
    "By comparing these two variances, the F-statistic assesses the likelihood that the observed differences between group means are due to chance or a real effect of the independent variable. A larger F-statistic suggests that the between-group variance is significantly larger than the within-group variance, indicating a strong effect of the independent variable.\n",
    "\n",
    "The partitioning of variance in ANOVA into between-group variance and within-group variance is central to calculating the F-statistic. Here's how it contributes:\n",
    "\n",
    "1. Between-Group Variance\n",
    "This measures the variability caused by differences between the group means. If the group means are very different, the between-group variance will be large, indicating a potential effect of the independent variable.\n",
    "\n",
    "2. Within-Group Variance\n",
    "This measures the variability within each group, attributed to random error or individual differences. It serves as a baseline to compare against the between-group variance.\n",
    "\n",
    "3. The Role in F-Statistic\n",
    "The F-statistic compares between-group variance to within-group variance.\n",
    "If the between-group variance is significantly larger than the within-group variance, it suggests that the group means are not all equal.\n",
    "If the two variances are similar, it indicates that any differences in group means are likely due to random variation.\n",
    "\n",
    "4. Interpretation\n",
    "A large F-statistic suggests strong evidence that at least one group mean differs significantly.\n",
    "A small F-statistic suggests no meaningful difference between the group means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d839c6-28a6-4108-8a6f-1d53ae3d1c5a",
   "metadata": {},
   "source": [
    "# 7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa473ab-a5c4-4c9f-aa1a-d4d39884766d",
   "metadata": {},
   "source": [
    "The classical (frequentist) and Bayesian approaches to ANOVA differ fundamentally in their philosophical underpinnings, how they handle uncertainty, and how they approach parameter estimation and hypothesis testing. Hereâ€™s a comparison:\n",
    "\n",
    "1. Treatment of Uncertainty\n",
    "Frequentist Approach:\n",
    "Uncertainty is expressed in terms of long-run frequencies.\n",
    "P-values quantify the probability of observing data as extreme as the sample, assuming the null hypothesis is true.\n",
    "Assumes fixed parameters with uncertainty only in the data.\n",
    "\n",
    "Bayesian Approach:\n",
    "Uncertainty is modeled directly in terms of probability distributions over parameters.\n",
    "Incorporates prior beliefs about parameters and updates them with observed data to produce posterior distributions.\n",
    "Parameters are treated as random variables with associated distributions.\n",
    "\n",
    "2. Parameter Estimation\n",
    "Frequentist Approach:\n",
    "Estimates parameters (e.g., group means and variances) using point estimates like sample means or maximum likelihood estimates.\n",
    "Confidence intervals provide a range of plausible values for parameters but depend on repeated sampling.\n",
    "\n",
    "Bayesian Approach:\n",
    "Produces posterior distributions for parameters, reflecting both prior knowledge and the observed data.\n",
    "Provides credible intervals, which are direct probabilities for parameters lying within a range (e.g., \"there's a 95% probability that the mean difference is within this range\").\n",
    "\n",
    "3. Hypothesis Testing\n",
    "Frequentist Approach:\n",
    "Focuses on null hypothesis significance testing (NHST).\n",
    "Tests whether all group means are equal (ð»0), using the F-statistic and p-value to decide whether to reject ð»0.\n",
    "Binary decision-making (reject or fail to reject ð»0).\n",
    "\n",
    "Bayesian Approach:\n",
    "Does not rely on p-values or null hypothesis testing.\n",
    "Compares models using posterior probabilities, Bayes factors, or credible intervals.\n",
    "Provides a nuanced interpretation of evidence, such as the probability of the null hypothesis being true.\n",
    "\n",
    "4. Role of Priors\n",
    "Frequentist Approach:\n",
    "Does not use priors; results depend only on the data.\n",
    "Assumes no prior knowledge about parameters.\n",
    "\n",
    "Bayesian Approach:\n",
    "Requires specifying prior distributions for parameters, which can significantly influence results, especially with limited data.\n",
    "Allows for incorporation of expert knowledge or previous studies.\n",
    "\n",
    "5. Output\n",
    "Frequentist Approach:\n",
    "Provides F-statistics, p-values, and confidence intervals.\n",
    "Offers binary decisions about rejecting the null hypothesis.\n",
    "\n",
    "Bayesian Approach:\n",
    "Provides posterior distributions for parameters and model probabilities.\n",
    "Allows for probabilistic statements (e.g., \"thereâ€™s a 90% probability that group 1â€™s mean is higher than group 2â€™s mean\").\n",
    "\n",
    "6. Computational Complexity\n",
    "Frequentist Approach:\n",
    "Computationally simpler and more widely used in traditional statistical analysis.\n",
    "Relies on closed-form solutions and standard test statistics.\n",
    "\n",
    "Bayesian Approach:\n",
    "Computationally intensive, often requiring simulation techniques like Markov Chain Monte Carlo (MCMC) to estimate posterior distributions.\n",
    "Increasingly feasible with modern computing power.\n",
    "\n",
    "7. Interpretation\n",
    "Frequentist Approach:\n",
    "Results are interpreted in terms of long-run behavior (e.g., the likelihood of observing such data under the null hypothesis).\n",
    "Focuses on \"rejecting or not rejecting\" the null hypothesis.\n",
    "\n",
    "Bayesian Approach:\n",
    "Results are interpreted probabilistically (e.g., \"this hypothesis is more likely given the data\").\n",
    "Provides richer insights into parameter uncertainty and the relative likelihood of competing hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2de37-91e0-4287-995e-e5df49ab1d2f",
   "metadata": {},
   "source": [
    "## 8. Question: You have two sets of data representing the incomes of two different professions:\n",
    "## Profession A: [48, 52, 55, 60, 62]\n",
    "## Profession B: [45, 50, 55, 52, 47] \n",
    "## Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfa6efe-b6fb-41ca-bc0c-34a1b2729e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.089171974522293\n",
      "4\n",
      "4\n",
      "Fail to reject the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats as stats\n",
    "\n",
    "profA=[48, 52, 55, 60, 62]\n",
    "profB=[45, 50, 55, 52, 47]\n",
    "fstats=np.var(profA)/np.var(profB)\n",
    "print(fstats)\n",
    "df1=len(profA)-1\n",
    "df2=len(profB)-1\n",
    "print(df1)\n",
    "print(df2)\n",
    "alpha=0.05\n",
    "crit_val=stats.f.ppf(q=1-alpha,dfn=df1,dfd=df2)\n",
    "if fstats>crit_val:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb8c1e-8769-48a1-9906-e470e87b9ac8",
   "metadata": {},
   "source": [
    "## 9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:\n",
    "## Region A: [160, 162, 165, 158, 164'\n",
    "## Region B: [172, 175, 170, 168, 174'\n",
    "## Region C: [180, 182, 179, 185, 183'\n",
    "## Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "## Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c3c4bb-16cc-4b01-8405-d6cfb1eeb3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87330316742101\n",
      "P-value: 2.8706641879370266e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "region_A = np.array([160, 162, 165, 158, 164])\n",
    "region_B = np.array([172, 175, 170, 168, 174])\n",
    "region_C = np.array([180, 182, 179, 185, 183])\n",
    "F_statistic, p_value = f_oneway(region_A, region_B, region_C)\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"P-value:\", p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
